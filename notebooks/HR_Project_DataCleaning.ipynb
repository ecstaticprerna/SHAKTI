{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da67b67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Input file not found: C:\\Users\\soulf\\Downloads\\archive\\HR_Employee_Attrition.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 153\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 153\u001b[0m     raw_df \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[0;32m    154\u001b[0m     cleaned_df \u001b[38;5;241m=\u001b[39m clean_data(raw_df)\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🚀 Final cleaned data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcleaned_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 42\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load and validate raw data\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(Config\u001b[38;5;241m.\u001b[39mINPUT_PATH):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mConfig\u001b[38;5;241m.\u001b[39mINPUT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(Config\u001b[38;5;241m.\u001b[39mINPUT_PATH)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Raw data loaded. Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Input file not found: C:\\Users\\soulf\\Downloads\\archive\\HR_Employee_Attrition.csv"
     ]
    }
   ],
   "source": [
    "############################\n",
    "### DATA CLEANING SCRIPT ###\n",
    "############################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import json\n",
    "\n",
    "# ======================\n",
    "# CONFIGURATION\n",
    "# ======================\n",
    "class Config:\n",
    "    INPUT_PATH = r\"C:\\Users\\soulf\\Downloads\\archive\\HR_Employee_Attrition.csv\"\n",
    "    OUTPUT_PATH = r\"C:\\Users\\soulf\\Downloads\\archive\\HR_Employee_Cleaned.pkl\"\n",
    "    REPORT_PATH = r\"C:\\Users\\soulf\\Downloads\\archive\\cleaning_report.json\"\n",
    "    \n",
    "    # Define special columns\n",
    "    TARGET = \"Attrition\"\n",
    "    ID_COL = \"EmployeeNumber\"\n",
    "    CONSTANT_COLS = [\"EmployeeCount\", \"StandardHours\", \"Over18\"]\n",
    "    DATE_COLS = []  # Add if temporal features exist\n",
    "    \n",
    "    # Ordinal feature encoding mappings\n",
    "    ORDINAL_FEATURES = {\n",
    "        \"Education\": [\"Below College\", \"College\", \"Bachelor\", \"Master\", \"Doctor\"],\n",
    "        \"JobSatisfaction\": [\"Low\", \"Medium\", \"High\", \"Very High\"],\n",
    "        \"WorkLifeBalance\": [\"Bad\", \"Good\", \"Better\", \"Best\"]\n",
    "    }\n",
    "    \n",
    "    WINSORIZE_LIMITS = (0.01, 0.01)  # 1% trimming on both ends\n",
    "\n",
    "# ======================\n",
    "# HELPER FUNCTIONS\n",
    "# ======================\n",
    "def load_data():\n",
    "    \"\"\"Load and validate raw data\"\"\"\n",
    "    if not os.path.exists(Config.INPUT_PATH):\n",
    "        raise FileNotFoundError(f\"Input file not found: {Config.INPUT_PATH}\")\n",
    "    \n",
    "    df = pd.read_csv(Config.INPUT_PATH)\n",
    "    print(f\"✅ Raw data loaded. Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def generate_cleaning_report(original_shape, new_shape, missing_info, dupe_info, outlier_info, constant_cols):\n",
    "    \"\"\"Generate JSON cleaning report\"\"\"\n",
    "    report = {\n",
    "        \"original_samples\": original_shape[0],\n",
    "        \"original_features\": original_shape[1],\n",
    "        \"final_samples\": new_shape[0],\n",
    "        \"final_features\": new_shape[1],\n",
    "        \"missing_values_handled\": missing_info,\n",
    "        \"duplicates_removed\": dupe_info,\n",
    "        \"outliers_handled\": outlier_info,\n",
    "        \"constant_columns_removed\": constant_cols,\n",
    "        \"data_loss_percentage\": round((1 - new_shape[0]/original_shape[0])*100, 2)\n",
    "    }\n",
    "    \n",
    "    with open(Config.REPORT_PATH, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    print(f\"⏩ Cleaning report saved to {Config.REPORT_PATH}\")\n",
    "\n",
    "# ======================\n",
    "# MAIN CLEANING PIPELINE\n",
    "# ======================\n",
    "def clean_data(df):\n",
    "    original_shape = df.shape\n",
    "    report_data = {}\n",
    "\n",
    "    # Phase 1: Initial Cleaning\n",
    "    # --------------------------\n",
    "    # Remove irrelevant columns\n",
    "    df = df.drop(columns=[Config.ID_COL] + Config.CONSTANT_COLS + Config.DATE_COLS, errors=\"ignore\")\n",
    "    report_data[\"constant_cols\"] = Config.CONSTANT_COLS\n",
    "    \n",
    "    # Convert columns to proper types\n",
    "    df[Config.TARGET] = df[Config.TARGET].astype(\"category\")\n",
    "    \n",
    "    # Phase 2: Handle Missing Values\n",
    "    # ------------------------------\n",
    "    missing_before = df.isna().sum().to_dict()\n",
    "    threshold = 0.3  # 30% missing threshold\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Remove high-missing columns\n",
    "        if df[col].isna().mean() > threshold:\n",
    "            df.drop(columns=col, inplace=True)\n",
    "            report_data.setdefault(\"high_missing_cols\", []).append(col)\n",
    "            continue\n",
    "            \n",
    "        # Impute remaining missing values\n",
    "        if pd.api.types.is_categorical_dtype(df[col]):\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "        else:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    report_data[\"missing_info\"] = {\n",
    "        \"before\": missing_before,\n",
    "        \"after\": df.isna().sum().to_dict()\n",
    "    }\n",
    "\n",
    "    # Phase 3: Handle Duplicates\n",
    "    # ---------------------------\n",
    "    duplicates = df.duplicated().sum()\n",
    "    df = df.drop_duplicates()\n",
    "    report_data[\"dupe_info\"] = {\n",
    "        \"duplicates_found\": duplicates,\n",
    "        \"remaining_samples\": df.shape[0]\n",
    "    }\n",
    "\n",
    "    # Phase 4: Outlier Handling\n",
    "    # --------------------------\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    outlier_report = {}\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        original_std = df[col].std()\n",
    "        df[col] = winsorize(df[col], limits=Config.WINSORIZE_LIMITS)\n",
    "        outlier_report[col] = {\n",
    "            \"original_std\": original_std,\n",
    "            \"new_std\": df[col].std(),\n",
    "            \"winsorized_percentage\": Config.WINSORIZE_LIMITS[0]*100*2\n",
    "        }\n",
    "    \n",
    "    report_data[\"outlier_info\"] = outlier_report\n",
    "\n",
    "    # Phase 5: Categorical Encoding\n",
    "    # ------------------------------\n",
    "    categorical_cols = df.select_dtypes(include=[\"category\", \"object\"]).columns.tolist()\n",
    "    ordinal_cols = [col for col in categorical_cols if col in Config.ORDINAL_FEATURES]\n",
    "    nominal_cols = list(set(categorical_cols) - set(ordinal_cols))\n",
    "    \n",
    "    # Ordinal Encoding\n",
    "    for col in ordinal_cols:\n",
    "        encoder = OrdinalEncoder(categories=[Config.ORDINAL_FEATURES[col]])\n",
    "        df[col] = encoder.fit_transform(df[[col]])\n",
    "    \n",
    "    # One-Hot Encoding (delayed until after EDA)\n",
    "    # Preserve original categorical columns for EDA\n",
    "    df = pd.get_dummies(df, columns=nominal_cols, drop_first=False, dummy_na=False)\n",
    "    \n",
    "    # Finalize\n",
    "    df.to_pickle(Config.OUTPUT_PATH)\n",
    "    generate_cleaning_report(original_shape, df.shape, \n",
    "                           report_data[\"missing_info\"], report_data[\"dupe_info\"],\n",
    "                           report_data[\"outlier_info\"], report_data[\"constant_cols\"])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    raw_df = load_data()\n",
    "    cleaned_df = clean_data(raw_df)\n",
    "    print(f\"🚀 Final cleaned data shape: {cleaned_df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
