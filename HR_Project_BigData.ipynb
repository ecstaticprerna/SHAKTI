{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdd3bda-87f3-41fc-8ca1-0aa2fb230ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT LIBRARIES AND INITIALIZE SPARK SESSION\n",
    "# =============================================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EmployeeAttritionPrediction\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "# =============================================================================\n",
    "# DATA LOADING\n",
    "# =============================================================================\n",
    "# Load the dataset\n",
    "df = spark.read.csv(\"HR_Employee_Cleaned.csv\", header=True, inferSchema=True)\n",
    "# Check schema and class distribution\n",
    "print(\"Schema:\")\n",
    "df.printSchema()\n",
    "print(\"\\nClass distribution:\")\n",
    "df.groupBy(\"Attrition\").count().show()\n",
    "# =============================================================================\n",
    "# PIPELINE PREPARATION\n",
    "# =============================================================================\n",
    "# Prepare feature vector\n",
    "feature_columns = [col for col in df.columns if col != \"Attrition\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "# =============================================================================\n",
    "# SPLITTING THE DATA\n",
    "# =============================================================================\n",
    "# Split data into training and test sets (70/30)\n",
    "train_data, test_data = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL TRAINING: RANDOM FOREST CLASSIFIER\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize Random Forest Classifier with class weighting\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"Attrition\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"classWeight\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Add class weights to handle imbalance (if needed)\n",
    "from pyspark.sql import functions as F\n",
    "class_weights = df.groupBy(\"Attrition\").count().rdd.collectAsMap()\n",
    "total = df.count()\n",
    "weight_udf = F.when(F.col(\"Attrition\") == 0, total / (2 * class_weights[0])) \\\n",
    "              .otherwise(total / (2 * class_weights[1]))\n",
    "df = df.withColumn(\"classWeight\", weight_udf)\n",
    "train_data = df.randomSplit([0.7, 0.3], seed=42)[0]  # Re-split after adding weights\n",
    "\n",
    "# Pipeline stages\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Hyperparameter grid for tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15, 20]) \\\n",
    "    .addGrid(rf.numTrees, [50, 100, 150, 200]) \\\n",
    "    .addGrid(rf.maxBins, [32, 64, 128]) \\\n",
    "    .addGrid(rf.minInfoGain, [0.0, 0.1, 0.2]) \\\n",
    "    .addGrid(rf.impurity, [\"gini\", \"entropy\"]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "# Evaluators (for metrics and cross-validation)\n",
    "multi_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Attrition\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "bin_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"Attrition\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Cross-validator with 3-fold cross-validation\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=bin_evaluator,\n",
    "    numFolds=5,  # More robust validation\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "cv_model = cv.fit(train_data)\n",
    "\n",
    "# Get best model and predictions\n",
    "best_model = cv_model.bestModel\n",
    "test_predictions = best_model.transform(test_data)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = multi_evaluator.evaluate(test_predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "precision = multi_evaluator.evaluate(test_predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = multi_evaluator.evaluate(test_predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = multi_evaluator.evaluate(test_predictions, {multi_evaluator.metricName: \"f1\"})\n",
    "auc = bin_evaluator.evaluate(test_predictions)\n",
    "\n",
    "# Feature importance from the Random Forest model\n",
    "feature_importances = best_model.stages[-1].featureImportances.toArray()\n",
    "features_importance_df = spark.createDataFrame(\n",
    "    [(feature, float(importance)) for feature, importance in zip(feature_columns, feature_importances)],\n",
    "    [\"Feature\", \"Importance\"]\n",
    ").orderBy(\"Importance\", ascending=False)\n",
    "# =============================================================================\n",
    "# FINAL EVALUATION & CLEANUP\n",
    "# =============================================================================\n",
    "# Display results\n",
    "print(\"\\nModel Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 Features Influencing Attrition:\")\n",
    "features_importance_df.show(10, truncate=False)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76265ed5-f9c5-48c6-b9dd-454dee37c7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# After running the Random Forest model code in HR_Project_BigData.ipynb\n",
    "\n",
    "# 1. Feature Importance Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "features_importance_pd = features_importance_df.toPandas()\n",
    "plt.barh(features_importance_pd['Feature'][:10], features_importance_pd['Importance'][:10])\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Top 10 Features Influencing Attrition (Random Forest)')\n",
    "plt.gca().invert_yaxis()  # Most important at top\n",
    "plt.show()\n",
    "\n",
    "# 2. Class Distribution Pie Chart\n",
    "class_dist = df.groupBy(\"Attrition\").count().toPandas()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(class_dist['count'], labels=['No Attrition', 'Attrition'], \n",
    "        autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])\n",
    "plt.title('Class Distribution in Dataset')\n",
    "plt.show()\n",
    "\n",
    "# 3. ROC Curve (if you want to plot it manually)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "# Get probabilities and true labels\n",
    "y_true = test_predictions.select(\"Attrition\").rdd.flatMap(lambda x: x).collect()\n",
    "y_prob = test_predictions.select(\"probability\").rdd.map(lambda x: float(x[0][1])).collect()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Random Forest ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
